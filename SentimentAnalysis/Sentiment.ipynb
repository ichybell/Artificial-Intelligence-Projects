{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a8ee4564",
   "metadata": {},
   "source": [
    "Created on 20th November 2022 at 18:00:00\n",
    "\n",
    "@ author IanPeter\n",
    "\n",
    "Dataset is credit to the paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015\n",
    "\n",
    "Dataset has 500 positive and 500 negative sentences with a clearly positive or negative connotaton. \n",
    "\n",
    "Goal of the paper was to have no neutral sentences selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc744efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26400e8d",
   "metadata": {},
   "source": [
    "# Preprocessing text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d94d937",
   "metadata": {},
   "source": [
    "- Converts text to lowercase\n",
    "- Tokenizes the text passed in\n",
    "- Removes stopwords\n",
    "- Perform stemming\n",
    "- Join text finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2333996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def cleanData(text):\n",
    "    text = text.lower() # All lowercase\n",
    "\n",
    "    # tokenize and remove stopwords at same time\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    new_tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    stemmed_tokens = [ps.stem(tokens) for tokens in new_tokens] # stemming\n",
    "\n",
    "    clean_text = \" \".join(stemmed_tokens)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6966b9",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa478779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the tab as a separator\n",
    "df_amazon = pd.read_csv('amazon_cells_labelled.txt', names=['sentence', 'score'], sep = '\\t', engine = 'python')\n",
    "\n",
    "# Converting the ndarray sentences into a list to perform easier operations with it\n",
    "train_sentences = df_amazon['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eafeab94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  score\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View sample from the data\n",
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164f3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing various classes needed\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "mn = MultinomialNB()\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8c71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning out the text from the training data.\n",
    "clean_text = [cleanData(i) for i in train_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2825d5",
   "metadata": {},
   "source": [
    "# Vectorization. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6411797",
   "metadata": {},
   "source": [
    "How many times the word was repeated in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "443fefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vect = cv.fit_transform(clean_text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f50f2",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62fe4429",
   "metadata": {},
   "source": [
    "Initialize the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e985f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to be used to test the model.\n",
    "df_yelp = pd.read_csv('yelp_labelled.txt', names=['sentence', 'score'], sep = '\\t', engine = 'python')\n",
    "\n",
    "# Get all scores in a list also\n",
    "train_score = df_amazon['score'].tolist() \n",
    "\n",
    "# Text to be used to test our data model.\n",
    "test_text = df_yelp['sentence'].tolist()\n",
    "\n",
    "# Performing cleaning on the test text\n",
    "clean_test_text = [cleanData(i) for i in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "994a5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization of the test text\n",
    "test_vect = cv.transform(clean_test_text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1a3d9",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "053251aa",
   "metadata": {},
   "source": [
    "Done using Multinomial Naive Bayes\n",
    "1 is positive and 0 is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6768e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data using Multinomial Naive Bayes\n",
    "mn.fit(sentence_vect,train_score)\n",
    "\n",
    "# Using the model to predict the score\n",
    "test_score = mn.predict(test_vect)\n",
    "\n",
    "# Getting the original score values for the test data\n",
    "original_score = df_yelp['score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21caab",
   "metadata": {},
   "source": [
    "### Evaluate performance "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2cdc93a",
   "metadata": {},
   "source": [
    "The accuracy_score function is then used to get the accuracy of the model as a percentage to 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da26bfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 70.20%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the model as a percentage\n",
    "accuracy = accuracy_score(original_score, test_score) * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy of the model: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4455f2",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad926eb5",
   "metadata": {},
   "source": [
    "Done using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e34a2cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 69.50%\n"
     ]
    }
   ],
   "source": [
    "# Fitting the data using Logistic Regression \n",
    "# Using model to predict score\n",
    "lr.fit(sentence_vect,train_score)\n",
    "test_score = lr.predict(test_vect)\n",
    "\n",
    "# Calculate the accuracy of the model as a percentage\n",
    "accuracy = accuracy_score(original_score, test_score) * 100\n",
    "\n",
    "# Print the confusion matrix and the accuracy\n",
    "print(f\"Accuracy of the model: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c349b92",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5a921fd",
   "metadata": {},
   "source": [
    "Even with various changes to either the classification method used or the use of lemattization instead of stemming, the model seems to peak at an accuracy of 70%\n",
    "Alternatively the following could be implemented in order to improve the model:\n",
    "\n",
    "1. Using different hyperparameters for the classification algorithm. Different hyperparameter settings can affect the performance of the model, and there may be a way to improve the performance by finding the optimal settings for this particular dataset.\n",
    "2. Using a larger dataset. With more data, the model may be able to learn more patterns and improve its performance.\n",
    "3. Using more advanced techniques, such as deep learning or transfer learning, to improve the performance of the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
